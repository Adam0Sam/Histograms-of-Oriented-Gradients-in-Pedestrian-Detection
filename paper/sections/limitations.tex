\section{Limitations}
The two fundamental limitations of this study lie in the amount of training data and available computational resources. 

Jain et al. proposed a rough guideline of having at least 10 times as many training data points as input dimensions \cite{jain_2000_statistical} so as to avoid the "curse of dimensionality" from section \ref{sec:curse_of_dimensionality}. Knowing that the largest possible and most frequent dimensional spaces that can be obtained from the set of values in table \ref{table:dependent_variables} is 175392 and 3780 respectively, as shown in figure \ref{fig:dimension_distribution} it becomes that the $\sim$ 27,000 data points from table \ref{table:dependent_variables} are theoretically insufficient to ensure that an SVM model trained on this data is always generalizable to new data.

During this investigation, resources were significantly conserved with the use of "stochastic" approach to training an SVM model. While section \ref{sec:svm_choice} does show no statistically significant difference between SGDClassifier and LinearSVC on a specific set of HOG parameters, there may be sets of parameters which do show a significant difference in performance between the two models. Resources were further conserved by only using 5 cross-validation folds across 4 regularization parameters, which may not be sufficient to ensure that the optimal regularization parameter was selected for each model.
